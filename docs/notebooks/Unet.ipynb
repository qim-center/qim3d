{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6781ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa88080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from os.path import join\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm, trange\n",
    "from monai.networks.nets import UNet\n",
    "from torchvision import transforms\n",
    "from monai.losses import FocalLoss, DiceLoss\n",
    "import qim3d\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a5eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for getting dataset path from string\n",
    "def get_dataset_path(name: str):\n",
    "    datasets = [\n",
    "        'belialev2020_side',\n",
    "        'gaudez2022_3d',\n",
    "        'guo2023_2d',\n",
    "        'stan2020_2d',\n",
    "        'reichardt2021_2d',\n",
    "        'testcircles_2dbinary',\n",
    "    ]\n",
    "    assert name in datasets, 'Dataset name must be ' + ' or '.join(datasets)\n",
    "\n",
    "    dataset_idx = datasets.index(name)\n",
    "\n",
    "    datasets_path = [\n",
    "        '/dtu/3d-imaging-center/projects/2023_STUDIOS_SD/analysis/data/Belialev2020/side',\n",
    "        '/dtu/3d-imaging-center/projects/2023_STUDIOS_SD/analysis/data/Gaudez2022/3d',\n",
    "        '/dtu/3d-imaging-center/projects/2023_STUDIOS_SD/analysis/data/Guo2023/2d/',\n",
    "        '/dtu/3d-imaging-center/projects/2023_STUDIOS_SD/analysis/data/Stan2020/2d',\n",
    "        '/dtu/3d-imaging-center/projects/2023_STUDIOS_SD/analysis/data/Reichardt2021/2d',\n",
    "        '/dtu/3d-imaging-center/projects/2023_STUDIOS_SD/analysis/data/TestCircles/2d_binary'\n",
    "    ]\n",
    "\n",
    "    return datasets_path[dataset_idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee235f48",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c088ceb8",
   "metadata": {},
   "source": [
    "### Check out https://albumentations.ai/docs/getting_started/transforms_and_targets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfef29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set transformation\n",
    "aug_train = A.Compose([\n",
    "    A.Resize(832,832),\n",
    "    A.RandomRotate90(),\n",
    "    A.Normalize(mean=(0.5),std=(0.5)), # Normalize to [-1, 1]\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Validation/test set transformation\n",
    "aug_val_test = A.Compose([\n",
    "    A.Resize(832,832),\n",
    "    A.Normalize(mean=(0.5),std=(0.5)), # Normalize to [-1, 1]\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766f0f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Possible datasets ####\n",
    "\n",
    "# 'belialev2020_side'\n",
    "# 'gaudez2022_3d'\n",
    "# 'guo2023_2d'\n",
    "# 'stan2020_2d'\n",
    "# 'reichardt2021_2d'\n",
    "# 'testcircles_2dbinary'\n",
    "\n",
    "# Choose dataset\n",
    "dataset = 'stan2020_2d'\n",
    "\n",
    "# Define class instances. First, both train and validation set is defined from train \n",
    "# folder with different transformations and below divided into non-overlapping subsets\n",
    "train_set = qim3d.qim3d.utils.Dataset(root_path=get_dataset_path(dataset),transform=aug_train)\n",
    "val_set = qim3d.qim3d.utils.Dataset(root_path=get_dataset_path(dataset),transform=aug_val_test)\n",
    "test_set = qim3d.qim3d.utils.Dataset(root_path=get_dataset_path(dataset),split='test',transform=aug_val_test)\n",
    "\n",
    "# Define fraction of training set used for validation\n",
    "VAL_FRACTION = 0.3\n",
    "split_idx = int(np.floor(VAL_FRACTION * len(train_set)))\n",
    "\n",
    "# Define seed\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "# Get randomly permuted indices \n",
    "indices = torch.randperm(len(train_set))\n",
    "\n",
    "# Define train and validation sets as subsets\n",
    "train_set = torch.utils.data.Subset(train_set, indices[split_idx:])\n",
    "val_set = torch.utils.data.Subset(val_set, indices[:split_idx])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "321495cc",
   "metadata": {},
   "source": [
    "### Data overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a794b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data has mask\n",
    "has_mask= False #True if train_set[0][-1] is not None else False\n",
    "\n",
    "print(f'No. of train images={len(train_set)}')\n",
    "print(f'No. of validation images={len(val_set)}')\n",
    "print(f'No. of test images={len(test_set)}')\n",
    "print(f'{train_set[0][0].dtype=}')\n",
    "print(f'{train_set[0][1].dtype=}')\n",
    "print(f'image shape={train_set[0][0].shape}')\n",
    "print(f'label shape={train_set[0][1].shape}')\n",
    "print(f'Labels={np.unique(train_set[0][1])}')\n",
    "print(f'Masked data? {has_mask}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5efa7d33",
   "metadata": {},
   "source": [
    "### Data visualization\n",
    "\n",
    "Display first seven image, labels, and masks if they exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170577d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qim3d.qim3d.viz.grid_overview(train_set,num_images=6,alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33368063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch sizes\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VAL_BATCH_SIZE = 4\n",
    "TEST_BATCH_SIZE = 4\n",
    "\n",
    "# Define dataloaders\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=VAL_BATCH_SIZE, num_workers=8, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=TEST_BATCH_SIZE, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35e83e38",
   "metadata": {},
   "source": [
    "# Train U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36685b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = UNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1, \n",
    "    out_channels=1, \n",
    "    channels=(64, 128, 256, 512, 1024), \n",
    "    strides=(2, 2, 2, 2), \n",
    ")\n",
    "\n",
    "orig_state = model.state_dict()  # Save, so we can reset model to original state later\n",
    "\n",
    "# Define loss function\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = FocalLoss()\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "137be29b",
   "metadata": {},
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d8a9f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "NUM_EPOCHS = 5\n",
    "EVAL_EVERY = 1\n",
    "PRINT_EVERY = 1\n",
    "LR = 3e-3\n",
    "\n",
    "\n",
    "model.load_state_dict(orig_state)  # Restart training every time\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "all_losses = []\n",
    "all_val_loss = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for data in train_loader:\n",
    "        if has_mask:\n",
    "            inputs, targets, masks = data\n",
    "            masks = masks.to(device).float()\n",
    "        else:\n",
    "            inputs, targets = data\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device).float().unsqueeze(1)\n",
    "        \n",
    "        # Forward -> Backward -> Step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        #print(f'input {inputs.shape}, target: {targets.shape}, output: {outputs.shape}')\n",
    "    \n",
    "        loss = loss_fn(outputs*masks, targets*masks) if has_mask else loss_fn(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.detach()\n",
    "        step += 1\n",
    "        \n",
    "    # Log and store average epoch loss\n",
    "    epoch_loss = epoch_loss.item() / step\n",
    "    all_losses.append(epoch_loss)\n",
    "\n",
    "    if epoch % EVAL_EVERY == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():  # Do not need gradients for this part\n",
    "            loss_sum = 0\n",
    "            step = 0\n",
    "            for data in val_loader:\n",
    "                if has_mask:\n",
    "                    inputs, targets, masks = data\n",
    "                    masks = masks.to(device).float()\n",
    "                else:\n",
    "                    inputs, targets = data\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device).float().unsqueeze(1)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                loss_sum += loss_fn(outputs*masks, targets*masks) if has_mask else loss_fn(outputs, targets)\n",
    "                step += 1\n",
    "                \n",
    "            val_loss = loss_sum.item() / step\n",
    "            all_val_loss.append(val_loss)\n",
    "\n",
    "        # Log and store average accuracy\n",
    "        if epoch % PRINT_EVERY == 0:\n",
    "            print(f'Epoch {epoch: 3}, train loss: {epoch_loss:.4f}, val loss: {val_loss:.4f}')\n",
    "\n",
    "print('Min val loss:', min(all_val_loss))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7a8e9d7",
   "metadata": {},
   "source": [
    "### Plot train and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851463c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 3))\n",
    "plt.plot(all_losses, '-', label='Train')\n",
    "plt.plot(all_val_loss, '-', label='Val.')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a700f8a",
   "metadata": {},
   "source": [
    "### Inspecting the Predicted Segmentations on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac83638",
   "metadata": {},
   "outputs": [],
   "source": [
    "qim3d.qim3d.viz.grid_pred(train_set,model,num_images=5,alpha=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a176ff96",
   "metadata": {},
   "source": [
    "### Inspecting the Predicted Segmentations on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb261c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qim3d.qim3d.viz.grid_pred(test_set,model,alpha=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
